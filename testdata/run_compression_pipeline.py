#!/usr/bin/env python3
"""
3-Layer Compression Pipeline Demo
Processes sample data through L1 (entities), L2 (communities), L3 (global)
"""

import json
import requests
from collections import defaultdict

AI_URL = "http://localhost:8001"
DGRAPH_URL = "http://localhost:8180"

def load_sample(path, limit=1000):
    """Load sample records from JSONL file."""
    records = []
    with open(path) as f:
        for i, line in enumerate(f):
            if i >= limit:
                break
            records.append(json.loads(line))
    return records

def layer1_local_parse(records):
    """L1: FAST local parsing - no LLM needed for structured data"""
    print(f"\n=== LAYER 1: Local Entity Parsing ({len(records)} records) ===")
    
    entities = []
    for r in records:
        # Extract entity directly from structured data
        entity = {
            "name": r.get("name", r.get("full_name", f"Entity-{r.get('id', 'unknown')}")),
            "type": r.get("type", "employee"),
            "department": r.get("department", "Unknown"),
            "role": r.get("role", ""),
            "skills": r.get("skills", []),
            "location": r.get("location", ""),
            "level": r.get("level", 0),
        }
        entities.append(entity)
    
    # Deduplicate by name
    unique = {}
    for e in entities:
        name = e.get("name", "Unknown")
        if name not in unique:
            unique[name] = e
        else:
            # Merge skills
            existing_skills = set(unique[name].get("skills", []))
            new_skills = set(e.get("skills", []))
            unique[name]["skills"] = list(existing_skills | new_skills)
    
    print(f"  Input records: {len(records)}")
    print(f"  Unique entities: {len(unique)}")
    print(f"  L1 compression: {len(records) / max(len(unique), 1):.1f}:1")
    print(f"  ⚡ Completed in <1 second (no LLM needed)")
    
    return list(unique.values())

def layer2_communities(entities):
    """L2: Group by department and summarize each community"""
    print(f"\n=== LAYER 2: Community Summarization ===")
    
    # Group by department
    by_dept = defaultdict(list)
    for e in entities:
        dept = e.get("department", "Unknown")
        by_dept[dept].append(e)
    
    print(f"  Communities found: {len(by_dept)}")
    
    summaries = []
    for dept, members in by_dept.items():
        resp = requests.post(
            f"{AI_URL}/summarize-community",
            json={
                "community_name": dept,
                "community_type": "department",
                "entities": members,
                "max_summary_length": 300
            },
            timeout=120
        )
        resp.raise_for_status()  # NO FALLBACK - fail if LLM unavailable
        summaries.append(resp.json())
        print(f"  ✓ {dept}: {len(members)} members (LLM summarized)")
    
    # Print top communities
    for s in sorted(summaries, key=lambda x: x.get("member_count", 0), reverse=True)[:5]:
        print(f"  - {s['community_name']}: {s.get('member_count', 0)} members")
    
    total_members = sum(e.get("member_count", 0) for e in summaries)
    print(f"  L2 compression: {total_members / max(len(summaries), 1):.1f}:1")
    
    return summaries

def layer3_global(summaries, total_entities):
    """L3: Generate global overview - NO FALLBACK, LLM REQUIRED"""
    print(f"\n=== LAYER 3: Global Overview ===")
    
    resp = requests.post(
        f"{AI_URL}/summarize-global",
        json={
            "namespace": "test-import",
            "community_summaries": summaries,
            "total_entities": total_entities
        },
        timeout=120
    )
    resp.raise_for_status()  # NO FALLBACK - fail if LLM unavailable
    overview = resp.json()
    print(f"  ✓ Global overview generated by LLM")
    
    print(f"\n{overview.get('title', 'Overview')}")
    print(f"  Total entities: {overview.get('total_entities', 0)}")
    print(f"  Communities: {overview.get('total_communities', 0)}")
    print(f"  Summary: {overview.get('executive_summary', 'N/A')[:200]}...")
    print(f"  Top skills: {', '.join(overview.get('top_skills', [])[:5])}")
    
    return overview

def calculate_compression(original_records, entities, summaries, overview):
    """Calculate compression ratios"""
    print("\n=== COMPRESSION RESULTS ===")
    
    # Estimate sizes
    original_size = sum(len(json.dumps(r)) for r in original_records)
    entity_size = sum(len(json.dumps(e)) for e in entities)
    summary_size = sum(len(json.dumps(s)) for s in summaries)
    overview_size = len(json.dumps(overview))
    
    total_compressed = entity_size + summary_size + overview_size
    
    print(f"  Original data: {original_size / 1024:.1f} KB ({len(original_records)} records)")
    print(f"  L1 entities:   {entity_size / 1024:.1f} KB ({len(entities)} entities)")
    print(f"  L2 summaries:  {summary_size / 1024:.1f} KB ({len(summaries)} communities)")
    print(f"  L3 overview:   {overview_size / 1024:.1f} KB (1 document)")
    print(f"  ────────────────────────────")
    print(f"  Total compressed: {total_compressed / 1024:.1f} KB")
    print(f"  Overall compression: {original_size / max(total_compressed, 1):.1f}:1")

def main():
    print("=" * 60)
    print("3-LAYER COMPRESSION PIPELINE DEMO")
    print("=" * 60)
    
    # Load ALL records from 1GB dataset (no limit)
    records = load_sample("testdata/large_dataset.jsonl", limit=999999999)
    
    # Layer 1: LOCAL PARSING (no LLM - instant)
    entities = layer1_local_parse(records)
    
    # Layer 2: Community summaries
    summaries = layer2_communities(entities)
    
    # Layer 3: Global overview
    overview = layer3_global(summaries, len(entities))
    
    # Calculate compression
    calculate_compression(records, entities, summaries, overview)
    
    print("\n✅ Pipeline complete!")

if __name__ == "__main__":
    main()
